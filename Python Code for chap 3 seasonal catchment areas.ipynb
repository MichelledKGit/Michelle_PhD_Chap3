{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python code for Chapter 3: SEASONAL CATCHMENT AREAS USING AN ATTRIBUTE BASED FUZZY LATTICE DATA STRUCTURE for toy example. Please note that the data used in POI catchment areas is not publically available and cannot be uploaded in this repository. Data ethics clearance number NAS003/2023.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n",
    "from functools import reduce\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
     "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB: The names in the structured nodes (1-9) are in the ordered format for V that the labelled states are first  i.e. grid_name={1,2,3,4,5,6,7,8,9}={v2,v7,v1,v3,v4,v5,v6,v8,v9}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Grid_Name  Grid_Name2  Absorbant_State  Absorbant_State2 Label_val\n",
      "0          1           1                1                 1        P1\n",
      "1          3           1                0                 1        U3\n",
      "2          4           1                0                 1        U4\n",
      "3          6           1                0                 1        U6\n",
      "4          2           2                1                 1        P2\n"
     ]
    }
   ],
   "source": [
    "#Import Grid_Neighbours_Labels. \n",
    "# Grid_Name and Grid_Name2 indicates the the structural nodes adjacent to each other. \n",
    "# Absorbant_State indicates if grid_name is absorbant(1) or not and same for Absorbant_State2 for grid_name2. \n",
    "# Label_Val indicates the label associated with Grid_Name where for the toy example, P1 and P1 indicates the two POIs and U3-U9 indicates unlabelled.\n",
    "base_path = '//cenana01/testcode/pythonUserVenv/michelled/Virtual_Environment/PhD/Chapter 3/'\n",
    "file_path = os.path.join(base_path, 'Grid_Neighbours_Labels.txt')\n",
    "\n",
    "DF = pd.read_csv(file_path, sep='|')\n",
    "print(DF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   grid_name attribute1\n",
      "0          1         A1\n",
      "1          2         A2\n",
      "2          3         A2\n",
      "3          4         A1\n",
      "4          5         A1\n"
     ]
    }
   ],
   "source": [
    "# Gives the attributes per grid. Each additional attribute will be added in another column the code is dynically set-up that way to pick up all distinct levels in all attributes provided\n",
    "#For toy example there is one attribute with two levels the level a2 is associated with structural nodes v1 and v7 and level a1 with v2,v3,v4,v5,v6,v8 and v9.\n",
    "file_path1 = os.path.join(base_path, 'Grid_Attributes.txt')\n",
    "\n",
    "Attributes = pd.read_csv(file_path1, sep='|')\n",
    "print(Attributes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   grid_Name  Absorbant_State  Total\n",
      "0          1                1      3\n",
      "1          2                1      2\n",
      "2          3                0      2\n",
      "3          4                0      2\n",
      "4          5                0      3\n"
     ]
    }
   ],
   "source": [
    "# Degrees File. Distinct on grid_name and give the total number of neighbouring structural nodes(total) and if it is an absorbent state\n",
    "file_path2 = os.path.join(base_path, 'Grid_Degrees.txt')\n",
    "\n",
    "# Read the file\n",
    "DF2 = pd.read_csv(file_path2, sep='|')\n",
    "print(DF2.head())\n",
    "\n",
    "absorbant_States=sum(DF2.Absorbant_State)\n",
    "nodes=len(DF2.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Using the information from DF create adjacency matrix \n",
    "df = pd.crosstab(DF.Grid_Name, DF.Grid_Name2)\n",
    "idx = df.columns.union(df.index)\n",
    "df = df.reindex(index = idx, columns=idx, fill_value=0)\n",
    "\n",
    "#Create correct format in Links table (new adjacency matrix)\n",
    "links = np.matrix(df).astype(np.float64)\n",
    "identity_abs = np.identity(absorbant_States)\n",
    "#Absorbent states should only indicate 1 to own state, nullify absorbent states and replace top left hand corner of matrix with identity matrix\n",
    "links[0:absorbant_States]=0\n",
    "#Replace the identity matrix in the top left hand corner of links matrix that all absorbent states can only move into themselves\n",
    "links[0:absorbant_States,0:absorbant_States]=identity_abs\n",
    "\n",
    "#Links is now the adjacency matrix but all absorbent states only has a probability of 1 to stay in the same state with the rest 0\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the attributes and create attribute matrix X with N the number of structural nodes, A the total number of attributes and T total number of unique levels over all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "1\n",
      "2\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Order by grid_name\n",
    "Attributes_sorted = Attributes.sort_values(by='grid_name')\n",
    "\n",
    "# Step 2: Drop grid_name\n",
    "Attributes_dropped = Attributes_sorted.drop(columns=['grid_name'])\n",
    "\n",
    "# Step 3: Stack remaining columns into attributes_raw\n",
    "attributes_raw = np.column_stack([Attributes_dropped[col].values for col in Attributes_dropped.columns])\n",
    "\n",
    "# Convert the attributes_raw matrix to a DataFrame\n",
    "df_attributes = pd.DataFrame(attributes_raw)\n",
    "df_attributes.columns = df_attributes.columns.astype(str)\n",
    "\n",
    "# Count the number of attributes (M)\n",
    "A = df_attributes.shape[1]\n",
    "N = links.shape[1]\n",
    "\n",
    "# Extract all unique values across all attributes\n",
    "distinct_values = np.unique(attributes_raw)\n",
    "\n",
    "# Count the number of distinct values in all attributes (ki)\n",
    "T = len(distinct_values)\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the categorical variables\n",
    "attributes_encoded = encoder.fit_transform(df_attributes).toarray()\n",
    "\n",
    "# Extract feature names from the encoder\n",
    "feature_names = encoder.get_feature_names_out(df_attributes.columns)\n",
    "\n",
    "# Create a new DataFrame with one-hot encoded columns\n",
    "df_encoded = pd.DataFrame(attributes_encoded, columns=feature_names)\n",
    "\n",
    "# Attribute matrix X\n",
    "X = df_encoded.values\n",
    "\n",
    "X = X.astype(np.float64)\n",
    "\n",
    "print(N)\n",
    "print(A)\n",
    "print(T)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5      0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.5      0.      ]\n",
      " [0.       0.5      0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.5     ]\n",
      " [0.333333 0.       0.       0.       0.333333 0.       0.       0.\n",
      "  0.       0.       0.333333]\n",
      " [0.333333 0.       0.       0.       0.       0.       0.333333 0.\n",
      "  0.       0.333333 0.      ]\n",
      " [0.       0.25     0.25     0.       0.       0.25     0.       0.\n",
      "  0.       0.25     0.      ]\n",
      " [0.2      0.       0.       0.       0.2      0.       0.2      0.2\n",
      "  0.       0.2      0.      ]\n",
      " [0.       0.       0.       0.25     0.       0.25     0.       0.\n",
      "  0.25     0.25     0.      ]\n",
      " [0.       0.25     0.       0.       0.       0.25     0.       0.\n",
      "  0.25     0.25     0.      ]\n",
      " [0.       0.       0.       0.       0.       0.       0.333333 0.333333\n",
      "  0.       0.333333 0.      ]\n",
      " [0.142857 0.       0.       0.142857 0.142857 0.142857 0.142857 0.142857\n",
      "  0.142857 0.       0.      ]\n",
      " [0.       0.5      0.5      0.       0.       0.       0.       0.\n",
      "  0.       0.       0.      ]]\n"
     ]
    }
   ],
   "source": [
    "#Structural weights, if all is 1 it it equal to the adjacency matrix\n",
    "w0=links\n",
    "# Create w with the weights of the attributes\n",
    "w = np.full(A, 1)\n",
    "\n",
    "#Shell for transition matrix\n",
    "transition_matrix = np.zeros((N + T, N + T))\n",
    "\n",
    "#Sum over the rows of structural weights w0\n",
    "row_sums = w0.sum(axis=1)\n",
    "row_sums=row_sums.reshape((N, 1))\n",
    "\n",
    "#sum over the rows of the attribute weights\n",
    "attributes_sums = X.T.sum(axis=1)\n",
    "attributes_sums=attributes_sums.reshape((T, 1))\n",
    "\n",
    "# Create shell for submatrices with zeros\n",
    "Matrix_Ps = np.zeros((N, N))\n",
    "Matrix_Psa_Structure = X\n",
    "Matrix_Pas=np.zeros((T,N))\n",
    "Matrix_Pas_Structure = X.T\n",
    "Matrix_0 = np.zeros((T, T))\n",
    "\n",
    "\n",
    "# Create a dictionary to map attribute values to their corresponding weights\n",
    "weights_dict = {}\n",
    "for i in range(A):\n",
    "    col_values = np.unique(df_attributes.iloc[:, i])\n",
    "    for val in col_values:\n",
    "        weights_dict[val] = w[i]\n",
    "\n",
    "# Create the row vector w with the corresponding weights\n",
    "wj = np.array([weights_dict[val] for val in distinct_values])\n",
    "sum_weights=np.sum(w)\n",
    "\n",
    "#From equations the values for submatrices\n",
    "Matrix_Ps=w0/(row_sums+sum_weights)\n",
    "\n",
    "Matrix_A=wj/(row_sums+sum_weights)\n",
    "Matrix_A = np.asarray(Matrix_A)\n",
    "Matrix_A *= Matrix_Psa_Structure\n",
    "\n",
    "Matrix_Psa_Structure = np.asarray(Matrix_Psa_Structure)\n",
    "\n",
    "Matrix_Pas=(1/attributes_sums) * Matrix_Pas_Structure\n",
    "\n",
    "transition_matrix=np.vstack((np.hstack((Matrix_Ps,Matrix_A)),np.hstack((Matrix_Pas,Matrix_0))))\n",
    "transition_matrix = np.round(transition_matrix, 6)\n",
    "\n",
    "#The Transition matrix shown in the article has the order v1,v2,v3,...,v9,va1,va2 to illustrate the probabilities in each row without confusing the reader and is also Pa before introducing absorbent states. \n",
    "#The structure in the code is different with the absorbant states begin first i.e row 1 and column 1 is node v2 and row 2 and column 2 is node V7 as this matrix was set up for the label propagation\n",
    "print(transition_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if matrix is stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is row stochastic? True\n"
     ]
    }
   ],
   "source": [
    "def is_row_stochastic(matrix):\n",
    "    # Check non-negativity\n",
    "    if np.any(matrix < 0):\n",
    "        return False\n",
    "    # Check if each row sum is within the range [0.99, 1.01]\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    return np.all((row_sums >= 0.99) & (row_sums <= 1.01))\n",
    "\n",
    "# Example usage:\n",
    "print(\"Is row stochastic?\", is_row_stochastic(transition_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0. ]\n",
      " [0.  0.5]]\n",
      "[[0.333333 0.      ]\n",
      " [0.333333 0.      ]\n",
      " [0.       0.25    ]\n",
      " [0.2      0.      ]\n",
      " [0.       0.      ]\n",
      " [0.       0.25    ]\n",
      " [0.       0.      ]\n",
      " [0.142857 0.      ]\n",
      " [0.       0.5     ]]\n",
      "[[0.       0.       0.333333 0.       0.       0.       0.       0.\n",
      "  0.333333]\n",
      " [0.       0.       0.       0.       0.333333 0.       0.       0.333333\n",
      "  0.      ]\n",
      " [0.25     0.       0.       0.25     0.       0.       0.       0.25\n",
      "  0.      ]\n",
      " [0.       0.       0.2      0.       0.2      0.2      0.       0.2\n",
      "  0.      ]\n",
      " [0.       0.25     0.       0.25     0.       0.       0.25     0.25\n",
      "  0.      ]\n",
      " [0.       0.       0.       0.25     0.       0.       0.25     0.25\n",
      "  0.      ]\n",
      " [0.       0.       0.       0.       0.333333 0.333333 0.       0.333333\n",
      "  0.      ]\n",
      " [0.       0.142857 0.142857 0.142857 0.142857 0.142857 0.142857 0.\n",
      "  0.      ]\n",
      " [0.5      0.       0.       0.       0.       0.       0.       0.\n",
      "  0.      ]]\n"
     ]
    }
   ],
   "source": [
    "# Partition the transition matrix into blocks as indicated on article\n",
    "P_ll = transition_matrix[:absorbant_States, :absorbant_States]\n",
    "P_ul = transition_matrix[absorbant_States:, :absorbant_States]\n",
    "P_uu = transition_matrix[absorbant_States:, absorbant_States:]\n",
    "\n",
    "print(P_ll)\n",
    "print(P_ul)\n",
    "print(P_uu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final probability matrix:\n",
      "[[1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.59390069 0.40609736]\n",
      " [0.78833178 0.21166501]\n",
      " [0.4847535  0.51524461]\n",
      " [0.66813817 0.33185965]\n",
      " [0.68802257 0.31197424]\n",
      " [0.49093962 0.50905798]\n",
      " [0.61864516 0.38135084]\n",
      " [0.67697515 0.32302144]\n",
      " [0.29695034 0.70304868]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the stationary distribution for the unlabeled nodes\n",
    "P_inf = inv(np.eye(P_uu.shape[0]) - P_uu).dot(P_ul)\n",
    "\n",
    "# Define the labels for the labeled nodes\n",
    "Y_l = np.identity(absorbant_States)  # Labels for node 1 and node 2\n",
    "\n",
    "# Calculate the labels for the unlabeled nodes\n",
    "Y_u = P_inf.dot(Y_l)\n",
    "\n",
    "# Combine the results for all nodes\n",
    "Y_all = np.vstack([Y_l, Y_u])\n",
    "\n",
    "# Print the final probability matrix\n",
    "print(\"Final probability matrix:\")\n",
    "print(Y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is row stochastic? True\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "print(\"Is row stochastic?\", is_row_stochastic(Y_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drivetimes: Please note for the toy example the same nodes with be indicated for drive-times 5, 10 and 15minutes but for real-world applications like the pharmacy sales more nodes will be included as the drive times extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take the first N rows as only the strctural nodes can be used for drivetimes\n",
    "L=Y_all[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   grid_name  store_grid\n",
      "0          1           1\n",
      "1          4           1\n",
      "2          6           1\n",
      "3          7           1\n",
      "4          2           2\n",
      "5          5           2\n",
      "6          6           2\n",
      "7          8           2\n"
     ]
    }
   ],
   "source": [
    "# The Grid_Name column indicates the grid that is allocated in the drive time circle of Store_Grid. I.e. grid names 1 and 2 are the only grids with a POI and which have a drivetime circle associated with it.\n",
    "file_path3 = os.path.join(base_path, 'Grids_to_Drivetimes_5min.txt')\n",
    "\n",
    "# Read the file\n",
    "DT5 = pd.read_csv(file_path3, sep='|')\n",
    "print(DT5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   grid_name  store_grid\n",
      "0          1           1\n",
      "1          4           1\n",
      "2          6           1\n",
      "3          7           1\n",
      "4          2           2\n",
      "5          5           2\n",
      "6          6           2\n",
      "7          8           2\n"
     ]
    }
   ],
   "source": [
    "# The Grid_Name column indicates the grid that is allocated in the drive time circle of Store_Grid. I.e. grid names 1 and 2 are the only grids with a POI and which have a drivetime circle associated with it.\n",
    "file_path4 = os.path.join(base_path, 'Grids_to_Drivetimes_10min.txt')\n",
    "\n",
    "# Read the file\n",
    "DT10 = pd.read_csv(file_path4, sep='|')\n",
    "print(DT10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   grid_name  store_grid\n",
      "0          1           1\n",
      "1          4           1\n",
      "2          6           1\n",
      "3          7           1\n",
      "4          2           2\n",
      "5          5           2\n",
      "6          6           2\n",
      "7          8           2\n"
     ]
    }
   ],
   "source": [
    "# The Grid_Name column indicates the grid that is allocated in the drive time circle of Store_Grid. I.e. grid names 1 and 2 are the only grids with a POI and which have a drivetime circle associated with it.\n",
    "file_path5 = os.path.join(base_path, 'Grids_to_Drivetimes_15min.txt')\n",
    "\n",
    "# Read the file\n",
    "DT15 = pd.read_csv(file_path5, sep='|')\n",
    "print(DT15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  2\n",
      "1  1  0\n",
      "2  0  1\n",
      "3  0  0\n",
      "4  1  0\n",
      "5  0  1\n",
      "6  1  1\n",
      "7  1  0\n",
      "8  0  1\n",
      "9  0  0\n"
     ]
    }
   ],
   "source": [
    "#5 minute drive time mappings\n",
    "#Create and indicator matrix with the columns the POIs and the rows indicating which nodes are in the drive-times of those POIs\n",
    "n_cols, n_rows = absorbant_States+1, nodes+1\n",
    "Dist5 = np.zeros((n_rows, n_cols), dtype=np.int32)\n",
    "\n",
    "Dist5[0,:] = np.arange(n_cols)\n",
    "Dist5[:,0] = np.arange(n_rows)\n",
    "\n",
    "d_selection5 = pd.DataFrame(DT5, columns = ['grid_name','store_grid'])\n",
    "\n",
    "for i in range(len(d_selection5)):\n",
    "    Dist5[d_selection5.loc[i, \"grid_name\"],d_selection5.loc[i, \"store_grid\"]]=1\n",
    "\n",
    "Dist5=pd.DataFrame(Dist5)\n",
    "Dist5 = Dist5.iloc[1: , 1:]\n",
    "Dist5=pd.DataFrame(Dist5)\n",
    "print(Dist5)\n",
    "\n",
    "myArray5 = np.where(Dist5==0, Dist5, L)\n",
    "myArray5[0:absorbant_States,0:absorbant_States]=identity_abs\n",
    "res5 = myArray5/myArray5.sum(axis=1)[:,None]\n",
    "res5=pd.DataFrame(res5)\n",
    "res5=res5.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  2\n",
      "1  1  0\n",
      "2  0  1\n",
      "3  0  0\n",
      "4  1  0\n",
      "5  0  1\n",
      "6  1  1\n",
      "7  1  0\n",
      "8  0  1\n",
      "9  0  0\n"
     ]
    }
   ],
   "source": [
    "#10 minute drive time mappings\n",
    "#Create and indicator matrix with the columns the POIs and the rows indicating which nodes are in the drive-times of those POIs\n",
    "n_cols, n_rows = absorbant_States+1, nodes+1\n",
    "Dist10 = np.zeros((n_rows, n_cols), dtype=np.int32)\n",
    "\n",
    "Dist10[0,:] = np.arange(n_cols)\n",
    "Dist10[:,0] = np.arange(n_rows)\n",
    "\n",
    "d_selection10 = pd.DataFrame(DT10, columns = ['grid_name','store_grid'])\n",
    "\n",
    "for i in range(len(d_selection10)):\n",
    "    Dist10[d_selection10.loc[i, \"grid_name\"],d_selection10.loc[i, \"store_grid\"]]=1\n",
    "\n",
    "Dist10=pd.DataFrame(Dist10)\n",
    "Dist10 = Dist10.iloc[1: , 1:]\n",
    "Dist10=pd.DataFrame(Dist10)\n",
    "print(Dist10)\n",
    "\n",
    "myArray10 = np.where(Dist10==0, Dist10, L)\n",
    "myArray10[0:absorbant_States,0:absorbant_States]=identity_abs\n",
    "res10 = myArray10/myArray10.sum(axis=1)[:,None]\n",
    "res10=pd.DataFrame(res10)\n",
    "res10=res10.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  2\n",
      "1  1  0\n",
      "2  0  1\n",
      "3  0  0\n",
      "4  1  0\n",
      "5  0  1\n",
      "6  1  1\n",
      "7  1  0\n",
      "8  0  1\n",
      "9  0  0\n"
     ]
    }
   ],
   "source": [
    "#15 minute drive time mappings\n",
    "#Create and indicator matrix with the columns the POIs and the rows indicating which nodes are in the drive-times of those POIs\n",
    "n_cols, n_rows = absorbant_States+1, nodes+1\n",
    "Dist15 = np.zeros((n_rows, n_cols), dtype=np.int32)\n",
    "\n",
    "Dist15[0,:] = np.arange(n_cols)\n",
    "Dist15[:,0] = np.arange(n_rows)\n",
    "\n",
    "d_selection15 = pd.DataFrame(DT15, columns = ['grid_name','store_grid'])\n",
    "\n",
    "for i in range(len(d_selection15)):\n",
    "    Dist15[d_selection15.loc[i, \"grid_name\"],d_selection15.loc[i, \"store_grid\"]]=1\n",
    "\n",
    "Dist15=pd.DataFrame(Dist15)\n",
    "Dist15 = Dist15.iloc[1: , 1:]\n",
    "Dist15=pd.DataFrame(Dist15)\n",
    "print(Dist15)\n",
    "\n",
    "myArray15 = np.where(Dist15==0, Dist15, L)\n",
    "myArray15[0:absorbant_States,0:absorbant_States]=identity_abs\n",
    "res15 = myArray10/myArray15.sum(axis=1)[:,None]\n",
    "res15=pd.DataFrame(res15)\n",
    "res15=res15.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supply and demand to calculate supply-demand ratio and accessibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   grid_name  Total\n",
      "0          1      1\n",
      "1          2      1\n",
      "2          3      1\n",
      "3          4      1\n",
      "4          5      1\n",
      "5          6      1\n",
      "6          7      1\n",
      "7          8      1\n",
      "8          9      1\n"
     ]
    }
   ],
   "source": [
    "# The Grid_Name column indicates the grid that is allocated in the drive time circle of Store_Grid. I.e. grid names 1 and 2 are the only grids with a POI and which have a drivetime circle associated with it.\n",
    "#The Demand for each grid in the toy example is 1\n",
    "file_path6 = os.path.join(base_path, 'Grid_Demand.txt')\n",
    "\n",
    "# Read the file\n",
    "Grid_Demand = pd.read_csv(file_path6, sep='|')\n",
    "\n",
    "# Convert grid_name to numeric if it's stored as string\n",
    "Grid_Demand['grid_name'] = Grid_Demand['grid_name'].astype(int)\n",
    "# Now sort\n",
    "Grid_Demand.sort_values('grid_name', inplace=True)\n",
    "Grid_Demand.reset_index(drop=True, inplace=True)\n",
    "print(Grid_Demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   grid_name  total_sales\n",
      "0          1            4\n",
      "1          2            4\n"
     ]
    }
   ],
   "source": [
    "# The Grid_Name column indicates the grid that is allocated in the drive time circle of Store_Grid. I.e. grid names 1 and 2 are the only grids with a POI and which have a drivetime circle associated with it.\n",
    "# The supply for the toy example for P1 and P2 is 4 \n",
    "file_path7 = os.path.join(base_path, 'Grid_Supply.txt')\n",
    "\n",
    "# Read the file\n",
    "Grid_Supply = pd.read_csv(file_path7, sep='|')\n",
    "print(Grid_Supply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid_name</th>\n",
       "      <th>Total_Grid_5min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   grid_name  Total_Grid_5min\n",
       "0          1             3.67\n",
       "1          2             3.33"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the total demand per grid based on the drive time rings: Demand at 5 minute drive-times\n",
    "Grid_Demand=pd.DataFrame(Grid_Demand)\n",
    "Grid_T=Grid_Demand.Total\n",
    "res5_T=res5.T\n",
    "\n",
    "Grid_Demand_5min=np.multiply(res5_T,Grid_T)\n",
    "Grid_Demand_5min=Grid_Demand_5min.T\n",
    "\n",
    "Total_Grid_Demand_5min=round(Grid_Demand_5min.sum(axis='rows'),2)\n",
    "Total_Grid_Demand_5min=pd.DataFrame(Total_Grid_Demand_5min)\n",
    "Total_Grid_Demand_5min.rename(columns={0:'Total_Grid_5min'})\n",
    "Total_Grid_Demand_5min.insert(0, 'grid_name', range(1, 1 + len(Total_Grid_Demand_5min)))\n",
    "Total_Grid_Demand_5min.rename(columns={0:'Total_Grid_5min'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid_name</th>\n",
       "      <th>Total_Grid_10min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   grid_name  Total_Grid_10min\n",
       "0          1              3.67\n",
       "1          2              3.33"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the total demand per grid based on the drive time rings: Demand at 10 minute drive-times\n",
    "Grid_Demand=pd.DataFrame(Grid_Demand)\n",
    "Grid_T=Grid_Demand.Total\n",
    "res10_T=res10.T\n",
    "\n",
    "Grid_Demand_10min=np.multiply(res10_T,Grid_T)\n",
    "Grid_Demand_10min=Grid_Demand_10min.T\n",
    "Total_Grid_Demand_10min=round(Grid_Demand_10min.sum(axis='rows'),2)\n",
    "Total_Grid_Demand_10min=pd.DataFrame(Total_Grid_Demand_10min)\n",
    "Total_Grid_Demand_10min.rename(columns={0:'Total_Grid_10min'})\n",
    "Total_Grid_Demand_10min.insert(0, 'grid_name', range(1, 1 + len(Total_Grid_Demand_10min)))\n",
    "Total_Grid_Demand_10min.rename(columns={0:'Total_Grid_10min'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid_name</th>\n",
       "      <th>Total_Grid_15min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   grid_name  Total_Grid_15min\n",
       "0          1              3.67\n",
       "1          2              3.33"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the total demand per grid based on the drive time rings: Demand at 15 minute drive-times\n",
    "Grid_Demand=pd.DataFrame(Grid_Demand)\n",
    "Grid_T=Grid_Demand.Total\n",
    "res15_T=res15.T\n",
    "\n",
    "Grid_Demand_15min=np.multiply(res15_T,Grid_T)\n",
    "Grid_Demand_15min=Grid_Demand_15min.T\n",
    "Total_Grid_Demand_15min=round(Grid_Demand_15min.sum(axis='rows'),2)\n",
    "Total_Grid_Demand_15min=pd.DataFrame(Total_Grid_Demand_15min)\n",
    "Total_Grid_Demand_15min.rename(columns={0:'Total_Grid_15min'})\n",
    "Total_Grid_Demand_15min.insert(0, 'grid_name', range(1, 1 + len(Total_Grid_Demand_15min)))\n",
    "Total_Grid_Demand_15min.rename(columns={0:'Total_Grid_15min'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   GRID_NAME  Total_Sales  Total_Demand_5min  Total_Demand_10min  \\\n",
      "0          1            4               3.67                3.67   \n",
      "1          2            4               3.33                3.33   \n",
      "\n",
      "   Total_Demand_15min  \n",
      "0                3.67  \n",
      "1                3.33  \n"
     ]
    }
   ],
   "source": [
    "# merge all drive times together and supply\n",
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['grid_name'],how='outer'), (Grid_Supply,Total_Grid_Demand_5min,Total_Grid_Demand_10min,Total_Grid_Demand_15min))        \n",
    "df_merged=pd.DataFrame(df_merged)\n",
    "df_merged.set_axis(['GRID_NAME', 'Total_Sales','Total_Demand_5min','Total_Demand_10min','Total_Demand_15min'], axis='columns', inplace=True)\n",
    "print(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   GRID_NAME  Total_Sales  Total_Demand_5min  Total_Demand_10min  \\\n",
      "0          1            4               3.67                3.67   \n",
      "1          2            4               3.33                3.33   \n",
      "\n",
      "   Total_Demand_15min  AR_5min  AR_10min  AR_15min  \n",
      "0                3.67     1.09      1.09      1.09  \n",
      "1                3.33     1.20      1.20      1.20  \n"
     ]
    }
   ],
   "source": [
    "#Add Supply-Demand Ratio for each drive-time. Since for the toy example all drivetimes are the same, the supply-demand ratio is also the same for 5,10 and 15min\n",
    "df_merged['Total_Sales'].fillna(0, inplace=True)\n",
    "df_merged['Total_Demand_5min'].fillna(0, inplace=True)\n",
    "\n",
    "df_merged['Total_Sales'] = pd.to_numeric(df_merged['Total_Sales'], errors='coerce')\n",
    "df_merged['Total_Demand_5min'] = pd.to_numeric(df_merged['Total_Demand_5min'], errors='coerce')\n",
    "\n",
    "df_merged['AR_5min']=df_merged['Total_Sales']/df_merged['Total_Demand_5min']\n",
    "df_merged['AR_10min']=df_merged['Total_Sales']/df_merged['Total_Demand_10min']\n",
    "df_merged['AR_15min']=df_merged['Total_Sales']/df_merged['Total_Demand_15min']\n",
    "\n",
    "df_merged=pd.DataFrame(df_merged)\n",
    "df_merged.fillna(0, inplace=True)\n",
    "df_merged.sort_values(['GRID_NAME'], inplace=True)\n",
    "df_merged = df_merged.round(2)\n",
    "\n",
    "print(df_merged)\n",
    "\n",
    "#The supply-demand ration for P1 is 1.09 and the supply-demand ration for P2 is 1.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the accessibility per grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Accessibility DataFrame:\n",
      "   Accessibility_5min  Accessibility_10min  Accessibility_15min\n",
      "1                1.09                 1.09                 1.09\n",
      "2                1.20                 1.20                 1.20\n",
      "3                0.00                 0.00                 0.00\n",
      "4                1.09                 1.09                 1.09\n",
      "5                1.20                 1.20                 1.20\n",
      "6                2.29                 2.29                 2.29\n",
      "7                1.09                 1.09                 1.09\n",
      "8                1.20                 1.20                 1.20\n",
      "9                0.00                 0.00                 0.00\n"
     ]
    }
   ],
   "source": [
    "Accessibility_5min = np.matmul(Dist5,df_merged.AR_5min)\n",
    "Accessibility_10min = np.matmul(Dist10,df_merged.AR_10min)\n",
    "Accessibility_15min = np.matmul(Dist15,df_merged.AR_15min)\n",
    "\n",
    "Accessibility = [Accessibility_5min,Accessibility_10min,Accessibility_15min]\n",
    "Accessibility=pd.DataFrame(Accessibility)\n",
    "Accessibility=Accessibility.T\n",
    "Accessibility.set_axis(['Accessibility_5min', 'Accessibility_10min', 'Accessibility_15min'], axis='columns', inplace=True)   \n",
    "\n",
    "# Round all values to 2 decimal places\n",
    "Accessibility = Accessibility.round(2)\n",
    "\n",
    "print(\"Updated Accessibility DataFrame:\")\n",
    "print(Accessibility)\n",
    "#The accessibility for grid v5 which in the ordered set V={v2,v7,v1,v3,v4,v5,v6,v8,v9} is grid_name 6 is 2.29\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Profiling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
